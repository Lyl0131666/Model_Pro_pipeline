# Model_Pro_pipeline

# Seismic Data Preprocessing Pipeline / åœ°éœ‡æ•°æ®é¢„å¤„ç†æµæ°´çº¿

æœ¬é¡¹ç›®åŒ…å«ä¸€å¥—å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ï¼Œæ—¨åœ¨å°†åŸå§‹çš„ MiniSEED åœ°éœ‡æ³¢å½¢æ•°æ®è½¬æ¢ä¸ºé€‚ç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ ‡å‡†åŒ– HDF5 æ•°æ®é›†ã€‚å¤„ç†æµç¨‹ä¸»è¦åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š**æ ¼å¼è½¬æ¢ä¸é¢„å¤„ç†**ã€**æ•°æ®æ ‡æ³¨**ä»¥åŠ**æ•°æ®åˆ‡ç‰‡**ã€‚

## ğŸ“‹ æµç¨‹æ¦‚è§ˆ (Pipeline Overview)

| é˜¶æ®µ | è„šæœ¬æ–‡ä»¶ | åŠŸèƒ½æè¿° | è¾“å…¥/è¾“å‡º |
| --- | --- | --- | --- |
| **Step 1** | `mseed2hdf5pro.py` | **æ ¼å¼è½¬æ¢ (Conversion)**<br>

<br>è¯»å–ä¸‰åˆ†é‡ mseed æ•°æ®ï¼Œå»é™¤ä»ªå™¨å“åº”ï¼Œå»å™ªï¼Œå¹¶è½¬æ¢ä¸º HDF5 æ ¼å¼ã€‚ | `.mseed` + XML â†’ `.hdf5` (Raw) |
| **Step 2** | `Labeling.py` | **æ ‡ç­¾ç”Ÿæˆ (Labeling)**<br>

<br>æ ¹æ®äº‹ä»¶ç›®å½•ï¼Œåœ¨ HDF5 æ–‡ä»¶ä¸­ç”Ÿæˆç‚¹å¯¹ç‚¹ (Point-wise) çš„è½¯æ ‡ç­¾ (0-1)ã€‚ | `.hdf5` (Raw) â†’ `.hdf5` (Labeled) |
| **Step 3** | `Cut_data_pro_5min.py` | **æ•°æ®åˆ‡ç‰‡ (Segmentation)**<br>

<br>å°†è¿ç»­æ•°æ®åˆ‡å‰²ä¸ºå›ºå®šé•¿åº¦çš„æ—¶é—´çª—ï¼Œå¯¹æ­£è´Ÿæ ·æœ¬é‡‡ç”¨ä¸åŒçš„é‡‡æ ·ç­–ç•¥ã€‚ | `.hdf5` (Labeled) â†’ `.hdf5` (Segments) |

---

## ğŸ› ï¸ è¯¦ç»†å¤„ç†æ­¥éª¤

### Step 1: æ ¼å¼è½¬æ¢ä¸é¢„å¤„ç† (Conversion)

**è„šæœ¬**: `mseed2hdf5pro.py`

è¯¥æ­¥éª¤è´Ÿè´£å°†åŸå§‹çš„åœ°éœ‡å°ç«™æ•°æ®ï¼ˆæŒ‰å¹´/å°ç«™/åˆ†é‡å­˜å‚¨çš„ `.mseed` æ–‡ä»¶ï¼‰æ•´åˆä¸ºå•ä¸€çš„ HDF5 æ–‡ä»¶ã€‚

* **æ ¸å¿ƒåŠŸèƒ½**:
1. **å¤šåˆ†é‡åˆå¹¶**: è‡ªåŠ¨åŒ¹é…åŒä¸€æ—¶é—´æ®µçš„ `EHZ`, `EHN`, `EHE` ä¸‰ä¸ªåˆ†é‡ã€‚
2. **ä¿¡å·é¢„å¤„ç†**:
* å»é™¤ä»ªå™¨å“åº” (Remove Instrument Response, Output=DISP)ã€‚
* å»çº¿æ€§è¶‹åŠ¿ (Detrend Linear) å’Œ å»å‡å€¼ (Demean)ã€‚


3. **å…ƒæ•°æ®æ³¨å…¥**: å°†é‡‡æ ·ç‡ã€å°ç«™ååŠ**åˆæ­¥äº‹ä»¶ä¿¡æ¯** (æ ¹æ® `events.txt` åŒ¹é…) å†™å…¥ HDF5 å±æ€§ä¸­ã€‚


* **è¾“å…¥ä¾èµ–**:
* åŸå§‹ mseed æ–‡ä»¶å¤¹ç»“æ„ã€‚
* `events.txt`: åŒ…å«äº‹ä»¶æ—¶é—´èŒƒå›´çš„ç›®å½•æ–‡ä»¶ã€‚
* Station XML æ–‡ä»¶ (`.xml`): ç”¨äºå»é™¤ä»ªå™¨å“åº”ã€‚



---

### Step 2: è½¯æ ‡ç­¾ç”Ÿæˆ (Labeling)

**è„šæœ¬**: `Labeling.py`

è¯»å– Step 1 ç”Ÿæˆçš„ HDF5 æ–‡ä»¶ï¼Œæ ¹æ®å…ƒæ•°æ®ä¸­çš„äº‹ä»¶æ—¶é—´ç”Ÿæˆå¯¹åº”çš„æ ‡ç­¾æ•°ç»„ (`labels`)ã€‚

* **æ ‡ç­¾ç­–ç•¥ (Soft Labeling)**:
ä¸ºäº†è®©æ¨¡å‹æ›´å¥½åœ°å­¦ä¹ åœ°éœ‡äº‹ä»¶çš„èµ·å§‹å’Œç»“æŸï¼Œé‡‡ç”¨æ¸è¿›å¼æ ‡ç­¾:
* **Event (1)**: äº‹ä»¶æŒç»­æœŸé—´ï¼Œæ ‡ç­¾å€¼ä¸º `1`ã€‚
* **Noise (0)**: éäº‹ä»¶æœŸé—´ï¼Œæ ‡ç­¾å€¼ä¸º `0`ã€‚
* **Transition (Ramp)**:
* **Fade-in**: äº‹ä»¶å¼€å§‹å‰ 60ç§’å†…ï¼Œæ ‡ç­¾ä» `0` çº¿æ€§å¢åŠ åˆ° `1`ã€‚
* **Fade-out**: äº‹ä»¶ç»“æŸå 60ç§’å†…ï¼Œæ ‡ç­¾ä» `1` çº¿æ€§å‡å°‘åˆ° `0`ã€‚





---

### Step 3: æ•°æ®åˆ‡ç‰‡ä¸å¢å¼º (Segmentation)

**è„šæœ¬**: `Cut_data_pro_5min.py`

å°†é•¿æ—¶é—´çš„è¿ç»­æ³¢å½¢åˆ‡å‰²ä¸ºå›ºå®šé•¿åº¦ï¼ˆä¾‹å¦‚ 5 åˆ†é’Ÿï¼‰çš„æ ·æœ¬ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒã€‚é’ˆå¯¹æ­£è´Ÿæ ·æœ¬ä¸å¹³è¡¡é—®é¢˜ï¼Œé‡‡ç”¨äº†åŠ¨æ€é‡å ç­–ç•¥ã€‚

* **åˆ‡ç‰‡é€»è¾‘**:
1. **æ­£æ ·æœ¬ (Events)**:
* å½“æ£€æµ‹åˆ° `labels > 0` æ—¶ï¼Œè§¦å‘**é«˜é‡å é‡‡æ ·**ã€‚
* **é‡å ç‡**: æ­¥é•¿ (Step Size) è®¾ç½®ä¸ºçª—å£é•¿åº¦çš„ 20% (å³ 80% é‡å )ï¼Œä»¥æ­¤å¢åŠ æ­£æ ·æœ¬æ•°é‡ (Data Augmentation)ã€‚


2. **è´Ÿæ ·æœ¬ (Noise)**:
* å½“æ•°æ®æ®µå†…æ— äº‹ä»¶æ—¶ï¼Œé‡‡ç”¨**æ— é‡å é‡‡æ ·**ã€‚
* æ­¥é•¿ç­‰äºçª—å£é•¿åº¦ï¼Œé¿å…äº§ç”Ÿè¿‡å¤šå†—ä½™çš„è´Ÿæ ·æœ¬ã€‚




* **è¾“å‡ºå†…å®¹**:
æ¯ä¸ªåˆ‡ç‰‡è¢«ä¿å­˜ä¸ºç‹¬ç«‹çš„ Groupï¼ŒåŒ…å« `data` (3é€šé“æ³¢å½¢), `labels` (ç‚¹æ ‡ç­¾), `binary_label` (äºŒåˆ†ç±»æ ‡ç­¾)ã€‚

---

## ğŸ“‚ ç›®å½•ç»“æ„ç¤ºä¾‹

```text
Project_Root/
â”œâ”€â”€ Raw_Data/                  # åŸå§‹è¾“å…¥
â”‚   â””â”€â”€ 2020/
â”‚       â””â”€â”€ Station_A/
â”‚           â”œâ”€â”€ EHZ/...mseed
â”‚           â”œâ”€â”€ EHN/...mseed
â”‚           â””â”€â”€ EHE/...mseed
â”œâ”€â”€ Processed_HDF5/            # Step 1 & 2 è¾“å‡º
â”‚   â””â”€â”€ JJG.Station_A.ALL.2020.136.hdf5 (åŒ…å« 'data' å’Œ 'labels')
â””â”€â”€ Cut_Data_Segments/         # Step 3 æœ€ç»ˆè¾“å‡º
    â””â”€â”€ JJG.Station_A.ALL.2020.136_all_segments.hdf5
        â”œâ”€â”€ segment_1/
        â”‚   â”œâ”€â”€ data (30000, 3)
        â”‚   â””â”€â”€ labels (30000,)
        â”œâ”€â”€ segment_2/
        â””â”€â”€ ...

```

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. è¿è¡Œæ ¼å¼è½¬æ¢ (éœ€ä¿®æ”¹è„šæœ¬ä¸­çš„è·¯å¾„é…ç½®)
python mseed2hdf5pro.py

# 2. ç”Ÿæˆæ ‡ç­¾
python Labeling.py

# 3. æ‰§è¡Œæ•°æ®åˆ‡ç‰‡
python Cut_data_pro_5min.py

```

---

`multi_watershed_preprocessor_v3.py` æ˜¯ä¸€ä¸ªé«˜åº¦é›†æˆçš„å¤„ç†å™¨ï¼Œå®ƒç›´æ¥è¯»å–åˆ†ç±»å¥½çš„æ³¢å½¢æ•°æ®ï¼ˆmseed/sacï¼‰ï¼Œæ‰§è¡Œé‡é‡‡æ ·ã€å»å™ªï¼Œå¹¶æ ¹æ®å°ç«™æ•°é‡è‡ªåŠ¨è°ƒæ•´åˆ‡ç‰‡ç­–ç•¥ï¼Œæœ€ç»ˆç”Ÿæˆç”¨äºæ·±åº¦å­¦ä¹ çš„ **HDF5 æ•°æ®é›†**ã€‚

å®ƒå®é™…ä¸Šåˆå¹¶å¹¶ä¼˜åŒ–äº†æ—§æµç¨‹ä¸­â€œæ ¼å¼è½¬æ¢â€å’Œâ€œæ•°æ®åˆ‡ç‰‡â€çš„æ­¥éª¤ï¼Œæ˜¯ä¸€ä¸ª **End-to-End** çš„é¢„å¤„ç†æ–¹æ¡ˆã€‚

---

## STEP 4: ç»Ÿä¸€é¢„å¤„ç†ä¸åˆ‡ç‰‡ (Unified Preprocessing & Segmentation)

è¿è¡Œè„šæœ¬ `multi_watershed_preprocessor_v3.py`ã€‚è¯¥è„šæœ¬è´Ÿè´£å°†ç»è¿‡ Step 1 å’Œ Step 2 æ•´ç†å¥½çš„è¿ç»­æ³¢å½¢æ•°æ®ï¼Œè½¬æ¢ä¸ºæ¨¡å‹å¯ç›´æ¥è¯»å–çš„ HDF5 åˆ‡ç‰‡æ–‡ä»¶ã€‚

### 1. æ ¸å¿ƒåŠŸèƒ½ (Key Features)

è¯¥è„šæœ¬ (v3ç‰ˆæœ¬) é’ˆå¯¹å¤šæµåŸŸã€å¤šæ ¼å¼æ•°æ®è¿›è¡Œäº†æ·±åº¦é€‚é…ï¼š

* **å¤šæ ¼å¼å…¼å®¹**:
* æ ‡å‡†æ ¼å¼: `Net.Sta.Comp.Year.DOY.mseed`
* å·è—çº¿æ ¼å¼: `YYYYMMDDHH_Station_Comp_merged.mseed`
* SAC æ ¼å¼: `Station...Comp.sac`


* **ç»Ÿä¸€é‡‡æ ·ç‡**: è‡ªåŠ¨å°†æ‰€æœ‰æ•°æ®é‡é‡‡æ ·è‡³ **100Hz**ã€‚
* **æ™ºèƒ½åˆ‡ç‰‡ç­–ç•¥ (Adaptive Segmentation)**:
* **å•å°ç«™æ¨¡å¼ (Single Station)**: å½“æŸæµåŸŸæŸå¹´åªæœ‰ä¸€ä¸ªå°ç«™æ—¶ï¼Œé‡‡ç”¨ **6åˆ†é’Ÿ (360s)** åˆ‡ç‰‡çª—å£ã€‚
* **å¤šå°ç«™æ¨¡å¼ (Multi Station)**: å½“å­˜åœ¨å¤šä¸ªå°ç«™ç»„ç½‘æ—¶ï¼Œé‡‡ç”¨ **5åˆ†é’Ÿ (300s)** åˆ‡ç‰‡çª—å£ï¼Œä»¥ä¾¿äºè¿›è¡Œç½‘ç»œååŒåˆ†æã€‚


* **é«˜æ•ˆå­˜å‚¨**: æ¯å¤©çš„æ‰€æœ‰åˆ‡ç‰‡å­˜å‚¨åœ¨ä¸€ä¸ªå•ä¸€çš„ `.h5` æ–‡ä»¶ä¸­ï¼Œé¿å…ç”Ÿæˆæ•°ç™¾ä¸‡ä¸ªå°æ–‡ä»¶ã€‚

### 2. è¿è¡Œè„šæœ¬

ä½¿ç”¨å‘½ä»¤è¡Œè¿è¡Œè„šæœ¬ï¼Œå¯ä»¥é€šè¿‡å‚æ•°çµæ´»é…ç½®è¾“å…¥è¾“å‡ºè·¯å¾„å’Œå¤„ç†æ¨¡å¼ã€‚

#### åŸºæœ¬è¿è¡Œ

```bash
python multi_watershed_preprocessor_v3.py \
  --source "E:\JJG_SORTED_DATA_2025_NORMALIZED" \
  --output "E:\JJG_HDF5_DATASET" \
  --parallel

```

#### å‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
| --- | --- | --- |
| `--source` | (ä»£ç å†…é»˜è®¤è·¯å¾„) | Step 2 ç”Ÿæˆçš„æ ‡å‡†åŒ–æ•°æ®æ ¹ç›®å½• |
| `--output` | (ä»£ç å†…é»˜è®¤è·¯å¾„) | HDF5 æ•°æ®é›†çš„è¾“å‡ºç›®å½• |
| `--parallel` | False | å¯ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç† (æ¨èå¼€å¯ï¼Œå¤§å¹…æå‡é€Ÿåº¦) |
| `--group-by` | `station` | è¾“å‡ºç›®å½•ç»“æ„ã€‚`station`: æŒ‰å°ç«™åˆ†æ–‡ä»¶å¤¹; `year`: æŒ‰å¹´ä»½åˆ†æ–‡ä»¶å¤¹ |
| `--dry-run` | False | ä»…æ‰«æå¹¶æ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸å®é™…ç”Ÿæˆæ–‡ä»¶ (ç”¨äºæ£€æŸ¥æ•°æ®) |
| `--segment-multi` | 300.0 | å¤šå°ç«™æ¨¡å¼ä¸‹çš„çª—å£é•¿åº¦ (ç§’) |
| `--segment-single` | 360.0 | å•å°ç«™æ¨¡å¼ä¸‹çš„çª—å£é•¿åº¦ (ç§’) |

### 3. å¤„ç†é€»è¾‘è¯¦è§£

è„šæœ¬å†…éƒ¨æ‰§è¡Œæµç¨‹å¦‚ä¸‹ï¼š

1. **æ‰«æ (Scanning)**: éå†ç›®å½•ï¼Œè‡ªåŠ¨è¯†åˆ«æµåŸŸ (Watershed)ã€å¹´ä»½å’Œå°ç«™ï¼Œå¹¶å»ºç«‹æ–‡ä»¶ç´¢å¼•ã€‚
2. **åˆ†ç»„ (Grouping)**: å°†åŒä¸€å¤©ã€åŒä¸€å°ç«™çš„ä¸åŒåˆ†é‡ï¼ˆZ/N/Eï¼‰æ–‡ä»¶èšåˆã€‚å¦‚æœåŒ…å«å°æ—¶çº§æ–‡ä»¶ï¼ˆå¦‚å·è—çº¿æ•°æ®ï¼‰ï¼Œä¼šè‡ªåŠ¨æŒ‰æ—¶é—´é¡ºåºæ’åºã€‚
3. **åŠ è½½ä¸é¢„å¤„ç† (Loading & Preprocessing)**:
* åˆå¹¶åˆ†æ®µæ³¢å½¢ã€‚
* **å»è¶‹åŠ¿ (Detrend)**: å»é™¤çº¿æ€§è¶‹åŠ¿å’Œå‡å€¼ã€‚
* **é‡é‡‡æ · (Resample)**: ç»Ÿä¸€è‡³ 100Hzã€‚


4. **åˆ‡ç‰‡ (Segmentation)**:
* æ ¹æ® `--overlap` (é»˜è®¤ 80%) è®¡ç®—æ­¥é•¿ã€‚
* æ»‘åŠ¨çª—å£æˆªå–æ•°æ®ã€‚


5. **å†™å…¥ (Writing)**: å°†åˆ‡ç‰‡æ•°æ®å†™å…¥ HDF5ï¼Œç»“æ„å¦‚ä¸‹ï¼š

```text
Output_HDF5_File.h5
â”œâ”€â”€ /Attributes (Global Metadata: Station, Year, SamplingRate...)
â”œâ”€â”€ /Watershed_Year_DOY_Station_seg000
â”‚   â”œâ”€â”€ data_Z (Array: 30000 points for 5min)
â”‚   â”œâ”€â”€ data_N
â”‚   â”œâ”€â”€ data_E
â”‚   â””â”€â”€ Attributes (Start_Time, Segment_Index)
â”œâ”€â”€ /Watershed_Year_DOY_Station_seg001
â””â”€â”€ ...

```

### 4. è¾“å‡ºç›®å½•ç»“æ„

è„šæœ¬è¿è¡Œå®Œæˆåï¼Œ`output` ç›®å½•ä¸‹å°†ç”Ÿæˆå¦‚ä¸‹ç»“æ„çš„æ•°æ®é›†ï¼ˆä»¥é»˜è®¤ `--group-by station` ä¸ºä¾‹ï¼‰ï¼š

```text
E:\JJG_HDF5_DATASET
â””â”€Jiangjiagou
    â””â”€2025
        â””â”€453007897
            â”œâ”€Jiangjiagou_2025_163_453007897_ALL.h5  (åŒ…å«è¯¥å°ç«™å½“å¤©çš„æ‰€æœ‰åˆ‡ç‰‡)
            â”œâ”€Jiangjiagou_2025_164_453007897_ALL.h5
            â””â”€...

```

### 5. ä¾èµ–ç¯å¢ƒ

é™¤äº†ä¹‹å‰çš„ä¾èµ–å¤–ï¼Œæœ¬æ­¥éª¤éœ€è¦ï¼š

* `h5py`: ç”¨äºè¯»å†™ HDF5 æ–‡ä»¶
* `pandas`: ç”¨äºç”Ÿæˆå¤„ç†æ—¥å¿—æ˜ å°„è¡¨
* `obspy`: æ ¸å¿ƒæ³¢å½¢å¤„ç†

```bash
pip install h5py pandas obspy

```

---

é€šè¿‡è¿è¡Œ `run_manager.py`ï¼Œæˆ‘ä»¬å°†æˆç™¾ä¸Šåƒä¸ªåˆ†æ•£çš„ HDF5 æ–‡ä»¶ç´¢å¼•åˆ°ä¸€ä¸ª **SQLite æ•°æ®åº“**ä¸­ï¼Œå¹¶ç”Ÿæˆç¬¦åˆæ¨¡å‹è¾“å…¥è¦æ±‚çš„â€œæ ·æœ¬å¯¹â€ï¼ˆSample Pairsï¼‰ã€‚è¿™ä½¿å¾— PyTorch `DataLoader` å¯ä»¥é«˜æ•ˆåœ°æŸ¥è¯¢å’ŒåŠ è½½æ•°æ®ï¼Œè€Œæ— éœ€åœ¨è®­ç»ƒ/é¢„æµ‹æ—¶éå†æ–‡ä»¶ç³»ç»Ÿã€‚

---

## STEP 5: æ•°æ®åº“æ„å»ºä¸æ ·æœ¬é…å¯¹ (Database Construction)

è¿è¡Œè„šæœ¬ `run_manager.py`ã€‚è¯¥è„šæœ¬è°ƒç”¨æ ¸å¿ƒåº“ `universal_debris_flow_database_manager_v2.py`ï¼Œå®Œæˆæ•°æ®çš„ç´¢å¼•ã€é€»è¾‘é…å¯¹å’Œæ•°æ®é›†åˆ’åˆ†ï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ª `.db` æ•°æ®åº“æ–‡ä»¶ã€‚

### 1. æ ¸å¿ƒæ¦‚å¿µ (Core Concepts)

è¯¥ç®¡ç†å™¨è§£å†³äº†æ³¥çŸ³æµç›‘æµ‹ä¸­**å•å°ç«™**ä¸**å¤šå°ç«™**æ•°æ®ä¸ç»Ÿä¸€çš„é—®é¢˜ï¼š

* **ç´¢å¼• (Indexing)**: è‡ªåŠ¨æ‰«æ Step 4 ç”Ÿæˆçš„ HDF5 æ–‡ä»¶ï¼Œè¯»å–å…ƒæ•°æ®ï¼ˆé‡‡æ ·ç‡ã€æ—¶é•¿ã€æ˜¯å¦åŒ…å«äº‹ä»¶ç­‰ï¼‰å­˜å…¥ SQLite `files` è¡¨ã€‚
* **é…å¯¹ (Pairing)**: æ¨¡å‹éœ€è¦æˆå¯¹çš„è¾“å…¥ï¼ˆä¸Šæ¸¸/ä¸‹æ¸¸ï¼‰ï¼Œç®¡ç†å™¨æ ¹æ®å°ç«™æ•°é‡è‡ªåŠ¨é€‰æ‹©ç­–ç•¥:
* **å¤šå°ç«™ (Physical Pairing)**: ç‰©ç†ä¸Šçš„ä¸Šæ¸¸å°ç«™ vs ä¸‹æ¸¸å°ç«™ã€‚
* **å•å°ç«™ (Virtual Self-Pairing)**: å°†åŒä¸€å°ç«™çš„æ•°æ®åœ¨æ—¶é—´ä¸Šé”™å¼€ï¼ˆä¾‹å¦‚å‰ 5 åˆ†é’Ÿä½œä¸ºâ€œä¸Šæ¸¸â€ï¼Œå 5 åˆ†é’Ÿä½œä¸ºâ€œä¸‹æ¸¸â€ï¼Œä¸­é—´é‡å  4 åˆ†é’Ÿï¼‰ï¼Œä»è€Œåˆ©ç”¨å•å°ç«™æ•°æ®é©±åŠ¨éœ€è¦åŒè¾“å…¥çš„æ¨¡å‹ã€‚


* **æ•°æ®é›†åˆ’åˆ† (Splitting)**: `run_manager.py` é»˜è®¤å°†æ‰€æœ‰æ•°æ®æ ‡è®°ä¸º `predict` (é¢„æµ‹é›†)ï¼Œç”¨äºæ¨¡å‹æ¨ç†ã€‚

### 2. é…ç½®è¿è¡Œ

æ‰“å¼€ `run_manager.py`ï¼Œæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹é…ç½®åŒºåŸŸï¼š

```python
# run_manager.py

# ================= é…ç½®åŒºåŸŸ =================
# è¾“å…¥ï¼šStep 4 ç”Ÿæˆçš„ HDF5 æ–‡ä»¶å¤¹è·¯å¾„
DATA_ROOT = r"/path/to/JJG_HDF5_DATASET"

# è¾“å‡ºï¼šç”Ÿæˆçš„ SQLite æ•°æ®åº“æ–‡ä»¶å
DB_PATH = "debris_flow_predict.db"

# é€šé“é…ç½®ï¼šæ ¹æ®æ¨¡å‹éœ€æ±‚é€‰æ‹© ['Z'] æˆ– ['Z', 'N', 'E']
CHANNELS = ['Z'] 

# é‡‡æ ·æ­¥é•¿ï¼šé¢„æµ‹æ—¶è®¾ä¸º 1 (ä¸è·³è¿‡ä»»ä½•æ•°æ®)
SKIP_STEP = 1
# ===========================================

```

### 3. è¿è¡Œè„šæœ¬

```bash
python run_manager.py

```

### 4. è¿è¡Œè¿‡ç¨‹è§£æ

è„šæœ¬æ‰§è¡Œæ—¶ä¼šç»å†ä»¥ä¸‹é˜¶æ®µï¼š

1. **åˆå§‹åŒ–**: æ¸…ç†æ—§çš„ `.db` æ–‡ä»¶ï¼Œå»ºç«‹æ–°çš„æ•°æ®åº“ Schema (åŒ…å« `files`, `segments`, `sample_pairs` è¡¨)ã€‚
2. **ç´¢å¼• (Indexing)**: éå† `DATA_ROOT` ä¸‹çš„æ‰€æœ‰æµåŸŸæ–‡ä»¶å¤¹ï¼Œå°†æ–‡ä»¶è·¯å¾„å’Œå…ƒæ•°æ®å†™å…¥æ•°æ®åº“ã€‚
3. **ç”Ÿæˆé…å¯¹ (Generating Pairs)**:
* å¯¹äºå•å°ç«™å¹´ä»½ï¼Œæ‰§è¡Œ `_generate_single_virtual_pairs`ã€‚
* å¯¹äºå¤šå°ç«™å¹´ä»½ï¼Œæ‰§è¡Œ `_generate_multi_physical_pairs`ã€‚


4. **æ ‡è®°é¢„æµ‹é›†**: å¼ºåˆ¶æ‰§è¡Œ SQL æ›´æ–°ï¼Œå°†æ‰€æœ‰æ ·æœ¬çš„ `dataset_split` è®¾ç½®ä¸º `'predict'`ï¼Œç¡®ä¿å®ƒä»¬èƒ½è¢«æ¨ç†ä»£ç è¯»å–ã€‚

### 5. è¾“å‡ºç»“æœ

è¿è¡Œå®Œæˆåï¼Œç›®å½•ä¸‹ä¼šç”Ÿæˆä¸€ä¸ª SQLite æ–‡ä»¶ï¼ˆä¾‹å¦‚ `debris_flow_predict.db`ï¼‰ã€‚
æ­¤æ•°æ®åº“åŒ…å«æ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰ç´¢å¼•ä¿¡æ¯ï¼š

* **`sample_pairs` è¡¨**: æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªå¯ä»¥ç›´æ¥è¾“å…¥æ¨¡å‹çš„æ ·æœ¬ï¼ŒåŒ…å«ï¼š
* `upstream_file_path`, `downstream_file_path`
* `upstream_slice_start`, `downstream_slice_start` (æ•°æ®åˆ‡ç‰‡ä½ç½®)
* `pair_type` (virtual_single / physical)



### 6. åœ¨æ¨¡å‹ä¸­ä½¿ç”¨ (Usage in PyTorch)

åœ¨æ‚¨çš„æ·±åº¦å­¦ä¹ é¢„æµ‹ä»£ç ä¸­ï¼Œæ— éœ€æ‰‹åŠ¨è¯»å–æ–‡ä»¶ï¼Œåªéœ€ä½¿ç”¨ç®¡ç†å™¨æä¾›çš„ Dataset ç±»ï¼š

```python
from universal_debris_flow_database_manager_v2 import UniversalDebrisFlowDatabaseManager, DebrisFlowPairDataset
from torch.utils.data import DataLoader

# 1. è¿æ¥æ•°æ®åº“
manager = UniversalDebrisFlowDatabaseManager('debris_flow_predict.db')

# 2. åˆ›å»º Dataset (æŒ‡å®šè¯»å– 'predict' é›†)
dataset = DebrisFlowPairDataset(
    db_manager=manager, 
    watershed='Jiangjiagou', 
    split='predict', 
    components=['Z']
)

# 3. åˆ›å»º DataLoader
loader = DataLoader(dataset, batch_size=32, shuffle=False)

# 4. è¿­ä»£æ•°æ®è¿›è¡Œé¢„æµ‹
for batch in loader:
    upstream = batch['upstream']     # Tensor: [Batch, 1, 30000]
    downstream = batch['downstream'] # Tensor: [Batch, 1, 30000]
    # output = model(upstream, downstream)

```
